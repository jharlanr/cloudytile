{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc7bdd6-dcde-4354-8173-3959eaaf2011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cfac50f-97cd-4353-8540-601240fc68be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "csv_file = \"/home/jupyter/repos/cloudytile/sandbox/labels.csv\"          # Path to your CSV\n",
    "img_dir = \"/home/jupyter/repos/cloudytile/data/2019cw_pngs/\"       # Directory containing PNGs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b1d96a9-5d65-4d64-84d1-8a49dad92847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Dataset Definition\n",
    "class PNGDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(row[\"label_num\"], dtype=torch.float32)\n",
    "        return image, label\n",
    "\n",
    "# 2) Transforms\n",
    "img_size = (128,128)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0899aad4-1d8d-4f84-8653-843fd0e22839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "img_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc5f3686-1cde-41e3-939b-35a9764fc2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) DataLoaders\n",
    "dataset = PNGDataset(csv_file, img_dir, transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c005983e-82c7-4ae4-8c5b-d837325a604e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_img = 1\n",
    "\n",
    "# 1) grab your batch\n",
    "images, labels = next(iter(val_loader))\n",
    "\n",
    "# 2) take the first image tensor\n",
    "img = images[idx_img].cpu().detach()   # shape (3, H, W)\n",
    "\n",
    "# 3) UN-normalize:  x_orig = x_norm * std + mean\n",
    "img = img * 0.5 + 0.5\n",
    "\n",
    "# 4) clamp to [0,1] just to be safe\n",
    "img = img.clamp(0,1)\n",
    "\n",
    "# 5) bring into HWC numpy for plotting\n",
    "img_np = img.permute(1,2,0).numpy()\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.title(f\"Label: {labels[idx_img].item()}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b72d8474-c9dc-42be-9494-491b5c8147e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) Model Definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (img_size[0]//8) * (img_size[1]//8), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9b83c0a-0da2-46fa-af47-f3e383b246bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3aed28-75ac-4b1d-8bff-7107bb8b01ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 0.5001 - Acc: 0.7534\n",
      " Val  - Loss: 0.3907 - Acc: 0.8554\n",
      "Epoch 2/100 - Loss: 0.3631 - Acc: 0.8396\n",
      " Val  - Loss: 0.3521 - Acc: 0.8614\n",
      "Epoch 3/100 - Loss: 0.3278 - Acc: 0.8533\n",
      " Val  - Loss: 0.3057 - Acc: 0.8554\n",
      "Epoch 4/100 - Loss: 0.2933 - Acc: 0.8578\n",
      " Val  - Loss: 0.2860 - Acc: 0.8614\n",
      "Epoch 5/100 - Loss: 0.2799 - Acc: 0.8578\n",
      " Val  - Loss: 0.2597 - Acc: 0.8675\n",
      "Epoch 6/100 - Loss: 0.2582 - Acc: 0.8638\n",
      " Val  - Loss: 0.2524 - Acc: 0.8614\n",
      "Epoch 7/100 - Loss: 0.2864 - Acc: 0.8366\n",
      " Val  - Loss: 0.2327 - Acc: 0.8614\n",
      "Epoch 8/100 - Loss: 0.2410 - Acc: 0.8578\n",
      " Val  - Loss: 0.2278 - Acc: 0.8614\n",
      "Epoch 9/100 - Loss: 0.2317 - Acc: 0.8714\n",
      " Val  - Loss: 0.2189 - Acc: 0.8614\n",
      "Epoch 10/100 - Loss: 0.2353 - Acc: 0.8820\n",
      " Val  - Loss: 0.2445 - Acc: 0.8313\n",
      "Epoch 11/100 - Loss: 0.7412 - Acc: 0.8064\n",
      " Val  - Loss: 1.2823 - Acc: 0.7169\n",
      "Epoch 12/100 - Loss: 1.1129 - Acc: 0.7595\n",
      " Val  - Loss: 0.3312 - Acc: 0.8494\n",
      "Epoch 13/100 - Loss: 0.3120 - Acc: 0.8563\n",
      " Val  - Loss: 0.2745 - Acc: 0.8614\n",
      "Epoch 14/100 - Loss: 0.2861 - Acc: 0.8548\n",
      " Val  - Loss: 0.2645 - Acc: 0.8614\n",
      "Epoch 15/100 - Loss: 0.2712 - Acc: 0.8578\n",
      " Val  - Loss: 0.2531 - Acc: 0.8735\n",
      "Epoch 16/100 - Loss: 0.2608 - Acc: 0.8699\n",
      " Val  - Loss: 0.2453 - Acc: 0.8614\n",
      "Epoch 17/100 - Loss: 0.2511 - Acc: 0.8623\n",
      " Val  - Loss: 0.2452 - Acc: 0.8494\n",
      "Epoch 18/100 - Loss: 0.2356 - Acc: 0.8865\n",
      " Val  - Loss: 0.2278 - Acc: 0.8614\n",
      "Epoch 19/100 - Loss: 0.2207 - Acc: 0.8729\n",
      " Val  - Loss: 0.2110 - Acc: 0.8976\n",
      "Epoch 20/100 - Loss: 0.2069 - Acc: 0.9092\n",
      " Val  - Loss: 0.1981 - Acc: 0.9036\n",
      "Epoch 21/100 - Loss: 0.2023 - Acc: 0.9092\n",
      " Val  - Loss: 0.2179 - Acc: 0.8735\n",
      "Epoch 22/100 - Loss: 0.1995 - Acc: 0.9047\n",
      " Val  - Loss: 0.1918 - Acc: 0.9277\n",
      "Epoch 23/100 - Loss: 0.1956 - Acc: 0.9062\n",
      " Val  - Loss: 0.1803 - Acc: 0.9518\n",
      "Epoch 24/100 - Loss: 0.1732 - Acc: 0.9395\n",
      " Val  - Loss: 0.1880 - Acc: 0.8795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    epoch_loss = train_loss / train_total\n",
    "    epoch_acc = train_correct / train_total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_epoch_loss = val_loss / val_total\n",
    "    val_epoch_acc = val_correct / val_total\n",
    "    print(f\" Val  - Loss: {val_epoch_loss:.4f} - Acc: {val_epoch_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad05cb-9e47-4d17-a0ca-a4e33a096dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
